{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load the sst2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# select the train split\n",
    "train = dataset[\"train\"]"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15a19addaa32494b90b8dbf82193dd53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17dba2c43470482989b114bd519ec7ba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /home/coder/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/7.44M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2719290d6180416297c6a030aeec048a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5726b4194d3c4cc1a318b716f88ed721"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2c8e0e4dfff4919859f256260dc1aff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7aad33a887fd41e7a6746ecf84bf0f04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dataset glue downloaded and prepared to /home/coder/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba3cd5e8a6fe40fcb74169f99d82030c"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(\"Examining train set:\")\n",
    "print(train)\n",
    "print(train.features)\n",
    "\n",
    "print(\"Information about the dataset:\")\n",
    "print(train.info.description)\n",
    "print(\"Homepage\")\n",
    "print(train.info.homepage)\n",
    "\n",
    "print(\"Examining sentence\")\n",
    "print(type(train[\"sentence\"]))\n",
    "print(type(train[\"sentence\"][0]))\n",
    "\n",
    "\n",
    "print(\"Examining label\")\n",
    "print(type(train[\"label\"]))\n",
    "print(type(train[\"label\"][0]))\n",
    "# set takes all the unique values\n",
    "print(set(train[\"label\"]))\n",
    "\n",
    "print(\"A few samples:\")\n",
    "for t in range(10):\n",
    "    sent = train[\"sentence\"][t]\n",
    "    lab = train[\"label\"][t]\n",
    "    print(sent, \"-\",  lab)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Examining train set:\nDataset({\n    features: ['sentence', 'label', 'idx'],\n    num_rows: 67349\n})\n{'sentence': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}\nInformation about the dataset:\nGLUE, the General Language Understanding Evaluation benchmark\n(https://gluebenchmark.com/) is a collection of resources for training,\nevaluating, and analyzing natural language understanding systems.\n\n\nHomepage\nhttps://datasets.stanford.edu/sentiment/index.html\nExamining sentence\n<class 'list'>\n<class 'str'>\nExamining label\n<class 'list'>\n<class 'int'>\n{0, 1}\nA few samples:\nhide new secretions from the parental units  - 0\ncontains no wit , only labored gags  - 0\nthat loves its characters and communicates something rather beautiful about human nature  - 1\nremains utterly satisfied to remain the same throughout  - 0\non the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up  - 0\nthat 's far too tragic to merit such superficial treatment  - 0\ndemonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .  - 1\nof saucy  - 1\na depressed fifteen-year-old 's suicidal poetry  - 0\nare more deeply thought through than in most ` right-thinking ' films  - 1\n"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'Hello': 0.125,\n 'friend': 0.25,\n '!': 0.125,\n 'Sara': 0.125,\n 'is': 0.125,\n 'my': 0.125,\n '.': 0.125}"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Hello friend! Sara is my friend.\"\n",
    "tokens = nlp(text)\n",
    "\n",
    "def term_freq(tokens): \n",
    "    '''\n",
    "    Takes in a list of tokens (str) and return a dictionary of term frequency for each token\n",
    "    '''\n",
    "    counts = Counter([token.text for token in tokens])\n",
    "    term_freq = {token: count/len(tokens) for (token, count) in counts.items()}\n",
    "    #print(term_freq)\n",
    "    return term_freq\n",
    "\n",
    "term_freq(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_frequency: sum over docs (for each doc return 1 if term in doc, 0 if term not in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.3-final",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python37364bite22ffb643747400a9f2b041f1845383e",
   "display_name": "Python 3.7.3 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}